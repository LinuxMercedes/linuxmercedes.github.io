<head>
<title>Technology Isn't Amoral</title>
<link href="../style.css"  rel="stylesheet" type="text/css">
</head>
<body>
<a href="..">Home</a> <a href=".">Writing</a>

<h2>Technology Isn't Amoral</h2>
<p>
It is common to see the opinion that "Technology is amoral" in technical discussions. 
At first glance, this argument seems sound: technology is basically just hammers made out of magnets, and hammers have both good and bad uses.
Therefore, technology must also not express a moral viewpoint; it is just a tool for its users to express their morals with. 
However, this argument goes wrong by greatly simplifying the situation. 
Technology is not necessarily amoral.
Many pieces of tech include at least part of the morals of their creators.
This will become more prevalent as we build more complex pieces of tech.
</p>

<p>
Technology is built to serve a purpose; we don't build things without some sort of reason. 
Therefore, the technology we have doesn't represent a random sampling from all possibly buildable things. 
This process of choosing what technology to build is how morality sneaks into our tech. 
Design decisions define the direction we take when developing technology; these decisions are ultimately based on our intuition, rather than purely technical merits. 
It's usually not obivous that these decisions are incorporating the authors' morals into their projects; as a result, few projects, if any, contain a complete or consistent morality. 
Anyone who has built complex systems know that the components interact in ways they would never have expected; this same complexity is responsible for self-contradictory moral judgements both in people and in their technology.
</p>

<p>
The morals embedded in technology are most visible in the tech that end users directly interact with.
Let's take a look at a few examples that will elucidate how these morals appear in technology.
</p>

<p>
IRC believes that peoples' interaction with it should be mostly controllable by those people themselves. 
As a result, users have their choice of client, with many client-side features including ignore lists and logging. 
Messages are (mostly) realtime and ephemeral-there is no replay, and therefore the server isn't required to do much more than forward messages to connected clients. <!-- How does this make the point I want to? -->
</p>

<p>
Twitter claims that there is value in expressing thoughts that are 140 or fewer characters long to everyone. 
The effect of this is that most tweets do not contain enough of an idea to be clearly taken out of context. 
Following people somewhat restores this context, as you have some sort of idea of what that person tweets about. 
However, retweets spread those tweets to people who don't follow the author, and hashtags remove tweets from their original context and place them into a new one (the feed for that hashtag). 
</p>

<p>
A third example: Whois believes that it is moral for domain owners to sacrifice their privacy for the freedom that having a domain brings (or, in certain cases, pay extra to maintain that privacy). 
You may argue that this is just a side effect of the age of when it was originally designed, but that is exactly the sort of subtle inclusion of morals we're looking at here. 
The designers worked under an assumption that their users would be primarily good or neutral. <!-- ...indicating something about their morality? -->
</p>

<p>
Finally, for a more hardware-centric example, consider wearable devices such as Google Glass, the FitBit, or <a href="https://www.fastcodesign.com/3047721/are-your-wearables-invading-someones-privacy">wearable cameras</a>.
All of these encode value in being constantly connected and monitored, usually with your data being stored in some proprietary data silo. 
In many of these cases, these devices signal not only this morality for the user, but also for people near the user who happen to be visible to the device. 
</p>

<p>
Why is it that it is particularly easy to point out morals encoded in user-facing technology?
I suspect it is due to the design decisions for those projects focusing primarily on people, instead of what the consuming layer of technology will want. 
These design decisions must involve choosing what is important to and good for people and good overall.

Something of a feedback loop is set up: developers make an initial design, perform user testing, gather feedback in the form of user stories, use case diagrams, and so forth, and go back to revise their design. <!-- What is the result of this? Patches to software morality? Introduced inconsistencies? -->
</p>

<p>
From these examples we can also see something of a trend for more complex technology to encode more visible moral judgements.
It's hard to imagine something technically simple like <code>cat</code> containing much of a worldview. 
I suspect this is due to larger projects requiring more value judgements&emdash;a larger 'morality surface', if you will.
</p>

<p>
Now, you may be concerned that we're conflating how people use technology with the technology itself.
However, it is impossible to truly separate these two concerns.
For example, plenty of people have worked around Twitter's 140 character limit by posting multipart tweets.
While that technically does give a means for expressing longer thoughts, Twitter's interface discourages this use; therefore, it still makes sense to discuss this moral choice in terms of Twitter's technology.
As well, a large amount of technology is moving to a software-as-a-service model, making it difficult to distinguish between the morals embedded in the techonlogy itself and the morals of the people running the service.
For instance, being able to block or ban abusers from a social network is both a technical problem and a people problem: software must allow for banning, and the people running the service must moderate and uphold these bans. 

<p>
So, what conclusions can we draw from this?
First off, it's reasonable to find contradictory moral judgements in tech projects.
These get introduced both by developers not realizing that they're sneaking pieces of their worldview into their projects and by users requesting new features without understanding the existing 'moral architecture' of the software. 
Second, malicious actors can use the morals embedded in a piece of technology to manipulate or abuse it, just as people can take advantage of the naievity or kind-heartedness of others.
We should distinguish between a technology being morally acceptable in spite of some people choosing to abuse it and a technology being morally acceptable despite being initially designed for malicious use.
<!--We should also criticize technology for having good intentions but failing to deliver on them. -->
Finally, as computational intelligence becomes more complex, it will encode more and more of the morals of its creators.
Let's make those moral decisions explicit and try to understand their side effects, rather than hoping our creations will reflect only the parts of us that we want them to. 
</p>

<!-- Scraps -->
<p>
For example, strong cryptography encodes the moral judgement that privacy is a human right (recall that every person, whether or not they are a criminal, is a human).
This right must be protected from anyone who would violate it, including both malicious actors and governments.
However, building backdoored cryptography encodes a significantly different moral judgement: privacy is a privilege granted by the government to those it deems worthy.
</p>

<p>
I don't think technology is 'morally turing-complete'; there are moral judgements that humans can make but cannot encode into tech.
</p>

<!--
Thesis: Technology is not necessarily amoral. 

- Definitions
	- Tech: hardware, software
	- Morality: differentiating between good and evil / good and bad / right and wrong // 'moral' means good/right
	- Value judgement: judging usefulness/goodness/rightness // inextricably linked to morality

- Tech reflects the morals/worldview of its makers
	- This is not to say that the authors of that tech fully understand the implications of their value judgements!
	- Tech must be designed; we aren't interested in all possible programs, just a subset that have been chosen for reasons
	- What those reasons are must be reflected in the tech itself by definition of choice
	- These decisions are made based on human intuition - a people-oriented process - not computable and provable logic
	- Few, if any, tech pieces contain a complete morality

- Morals in tech are most visible in stuff designed to interact directly with people
	- infrastructure folks, you can rest assured that no matter how radical your politics, you won't overthrow the world
	- Examples:
		- IRC: people's interaction should ultimately be mostly controllable by those people themselves: ignore lists, client logging, client agnosticity, etc. Messages are ephemeral (server is tiny, etc) and realtime (or close to it)
		- Twitter: There is value in expressing thoughts in ~140 chars / thoughts that small are meaningful. 'Follow' implies context to some tweets (compare to, say, yik yak), but retweets give a ~log(# RTs) separation from context, and hashtags when used as intended strip all personal context entirely (but add content context). 
		- Whois says it's moral/ok for domain owners to pay for the freedom that having a domain entails with their privacy (or additional $$ for workarounds)
		- Wearables/Glass encode value in always being connected / massive data collection and storage, almost exclusively in a proprietary silo
	- Why is this? Probably because design decisions made 'close to' people (as opposed to having multiple layers stacked on top of it before an end user will touch it) involve understanding/imposing a view of how those people work/what they want/what is good for them and good overall
	- Something of a feedback loop: Tech design, user testing, feedback (user stories, use case diagrams, etc), updated tech

- Less firm statements / Vaguer ideas
	- A lot of tech encoding morality is very big (platforms, etc)
	- Is there some sort of tipping point wrt project size where including moral judgements becomes inevitable? Where is that tipping point?
	- AI /must/ include morals of its creators, whether explicitly encoded or implicitly designed - it is both big and very user-centric!

- TODO: Is 'how we use technology' materially different from that technology itself? (perhaps address at end)
	- Certain technologies preclude/heavily discourage non-standard uses (e.g. twitter and longform pieces, but see storify or whatev; also whois)
	- https://twitter.com/TinkerSec/status/614117147955077120
	- A lot of tech nowadays is SaaS-ish and therefore comes with the baggage of people/corporation, for better or worse (see both banning abusers and the whole real-name policy BS)
-->
</body>

