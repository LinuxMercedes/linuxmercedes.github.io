<head>
<title>Technology Isn't Amoral</title>
<link href="../style.css"  rel="stylesheet" type="text/css">
</head>
<body>
<a href="..">Home</a> <a href=".">Writing</a>

<h2>Technology Isn't Amoral</h2>
<small><a href="me@nathanjar.us"><i>Nathan Jarus</i></a></small>
<!--
	List of concerns:
	- Lots of hedge words
	- Are 'design decisions' equivalent to 'moral judgements'?
	- AGPL is related to SaaS, maybe include?
-->
<p>
In technical discussions, people frequently express the opinion that "technology doesn't have morals" or "technology is amoral".
At first glance, this argument seems sound: technology is basically just hammers made out of magnets, and hammers have both good and bad uses&mdash;they are amoral.
Therefore, technology must also not express a moral viewpoint; it is just a tool for its users to express their morals with. 
However, this argument greatly simplifies the situation: technology is not necessarily amoral.
Many pieces of tech include at least part of the morals of their creators, and these morals will appear more and more frequently as we build more complex pieces of tech.
</p>

<p>
Technology is built to serve a purpose; we don't build things without some sort of reason. 
Therefore, the technology we have doesn't represent a random sampling from all possible buildable things&mdash;people choose to build things. 
This process of choosing what technology to build is how morality sneaks into our tech. 
Design decisions define the direction we take when developing technology; some of these decisions are ultimately based on our intuition instead of the technical merits of the choices.
It's usually not obvious that these decisions are incorporating the authors' morals into their projects; as a result, few projects, if any, contain a complete or consistent morality. 
Anyone who has built complex systems know that the components interact in ways they would never have expected; this same complexity is responsible for self-contradictory moral judgements both in people and in their technology.
</p>

<p>
What do morals embedded in technology look like, and where do they appear?
They're most visible in the tech that end users directly interact with.
Let's look at a few examples:
</p>

<p>
IRC believes that people's interaction with it should be mostly controllable by those people themselves. 
As a result, users have their choice of client, with many client-side features including ignore lists and logging. 
Messages are mostly real-time and ephemeral&mdash;there is no replay, and therefore the server isn't required to do much more than forward messages to connected clients. 
Users don't have to worry about the server relaying messages to people who aren't there in the room the message was said in for the most part. 
The end result of a client-agnostic server with straightforward message relay policies is that users have a lot of autonomy over how they interact with people and who hears what they say.
</p>

<p>
Twitter claims that there is value in expressing thoughts that are 140 or fewer characters long to everyone. 
The effect of this is that most tweets do not contain enough of an idea to be clearly taken out of context. 
Following people restores some of this context in an implicit fashion, as you have some sort of idea of what that person thinks. 
However, retweets spread those tweets to people who don't follow the author, and hashtags remove tweets from their original context and place them into a new one (the feed for that hashtag). 
The end result is that it is difficult or impossible to clearly communicate complex ideas to a wide audience or to discuss a disagreement because these situations rely on explicitly stated context. 
</p>

<p>
A third example: Whois believes that it is moral for domain owners to sacrifice their privacy for the freedom that having a domain brings (or, in certain cases, pay extra to maintain that privacy). 
The designers worked under an assumption that people who used whois data would be good or neutral and wouldn't abuse that information.
Therefore, privacy and anonymity were not, and <a href="https://www.icann.org/news/blog/on-whois-privacy-proxy-services">still are not</a>, necessary for the service.
You may argue that this is just a side effect of the age of when it was originally designed, but that is exactly what we are talking about&mdash;the designers made decisions based on their moral view of how other people would act.
</p>

<p>
Strong cryptography encodes the moral judgement that privacy is the right of every human.
However, building backdoored cryptography encodes a significantly different moral judgement: privacy is a privilege granted by the backdoor controller to those it deems worthy.
Both strong cryptography and backdoored cryptography believe that they cannot compromise the privacy of bad actors without compromising the privacy of every one of their users.
They differ on the moral judgement drawn from this belief: strong cryptography holds privacy above all else, while backdoored cryptography trusts the controller of that backdoor to uphold the privacy of its users; that is, backdoored cryptography believes its controller holds the same morals as its users. 
</p>

<p>
Finally, for a more hardware-centric example, consider devices such as Google Glass, <a href="http://fusion.net/story/158292/fitbit-data-just-undermined-a-womans-rape-claim/">fitness trackers</a>, or <a href="https://www.fastcodesign.com/3047721/are-your-wearables-invading-someones-privacy">wearable cameras</a>.
All of these encode value in being constantly connected and monitored, usually with your data being stored in some proprietary data silo. 
In many of these cases, these devices signal a moral choice the user has made not only about themselves but also about people near the user who happen to be visible to the device. 
Notice how easily the value judgements of the creators of these pieces of technology are applied to a group of people that expands far beyond the users of that product. 
</p>

<p>
From these examples we can see a trend of more complex technology encoding more visible moral judgements.
It's hard to imagine something technically simple like <code>cat</code> containing much of a morality. 
This is at least partly due to larger projects requiring more value judgements&mdash;a larger 'morality surface', if you will.
It is also caused by unforeseen interactions between smaller morals encoded in separately-built pieces of the technology. <!-- this is rather unsubstantiated -->
</p>

<p>
Why is it that it is particularly easy to point out morals encoded in user-facing technology?
One possible reason is that the design decisions for those projects focus primarily on people, instead of what a consuming layer of technology will want. 
These design decisions must involve choosing what is important to, and good for, people, as well as what is good overall.
Something of a feedback loop is set up: developers make an initial design, perform user testing, gather feedback in the form of user stories, use case diagrams, and so forth, and go back to revise their design. 
This feedback loop amplifies the moral judgements of both the creators and the users who give feedback.
It can also introduce moral inconsistencies when a particular design decision is reversed without considering the moral implications of that reversal.
(Consider Twitter's <a href="https://twittercommunity.com/t/removing-the-140-character-limit-from-direct-messages/41348">decision to remove the 140 character limit on direct messages</a>.)
</p>

<p>
Now, you may be concerned that we're conflating how people use technology with the technology itself.
However, it is impossible to truly separate these two concerns.
Plenty of people have worked around Twitter's 140 character limit, for instance, by posting multipart tweets.
While that technically does give a means for expressing longer thoughts, Twitter's interface discourages this use; therefore, it still makes sense to discuss this moral choice in terms of Twitter's technology.
As well, a large amount of technology is moving to a software-as-a-service model, making it meaningless to distinguish between the morals embedded in the technology itself and the morals of the people running the service.
For example, being able to block or ban abusers from a social network is both a technical problem and a people problem: software must allow for banning, and the people running the service must moderate and uphold these bans. 

<p>
So, what conclusions can we draw from this?
First, it's reasonable to find contradictory moral judgements in tech projects.
These get introduced both by developers not realizing that they're sneaking pieces of their worldview into their projects and by users requesting new features without understanding the existing 'moral architecture' of the software. 
Second, malicious actors can use the morals embedded in a piece of technology to manipulate or abuse it, just as people can take advantage of the naivety or kind-heartedness of others.
We should distinguish between a technology being morally acceptable in spite of some people choosing to abuse it and a technology being morally acceptable despite being initially designed for malicious use.
Furthermore, it is likely that technology isn't 'morally Turing-complete'; there are moral judgements that humans can make but do not know how to encode into tech.
Understanding what morality we can and cannot expect our technology to hold for us is a prerequisite for making good design decisions.
<!--We should also criticize technology for having good intentions but failing to deliver on them.-->
Finally, as computational intelligence becomes more complex, it will encode more and more of the morals of its creators.
Let's make those moral decisions explicit and try to understand their side effects, rather than hoping our creations will reflect only the parts of us that we want them to. 
</p>

<h4>Further Reading</h4>
<ul>
	<li><a href="https://theconversation.com/our-love-of-technology-risks-becoming-a-quiet-conspiracy-against-ourselves-43428">Our love of technology risks becoming a quiet conspiracy against ourselves</a></li>
	<li><a href="http://crookedtimber.org/2012/06/25/seeing-like-a-geek/">Seeing Like a Geek</a></li>
	<li><a href="http://www.theatlantic.com/technology/archive/2015/06/the-internet-of-things-you-dont-really-need/396485/">The Internet of Things You Don't Really Need</a></li>
	<li><a href="https://medium.com/bad-words/the-servitude-bubble-c9e998c437c6">The Servitude Bubble</a></li>
	<li><a href="http://jeremykun.com/2015/07/13/what-does-it-mean-for-an-algorithm-to-be-fair/">What does it mean for an algorithm to be fair?</a></li>
	<li><a href="http://www.nytimes.com/2015/07/10/upshot/when-algorithms-discriminate.html">When Algorithms Discriminate</a></li>
	<li><a href="http://www.theverge.com/2015/7/7/8905037/google-ad-discrimination-adfisher">Google's algorithms advertise higher paying jobs to more men than women</a></li>
</ul>

<small>This article is released under the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">cc-by-nc-sa</a> license.</small>
<!--
Thesis: Technology is not necessarily amoral. 

- Definitions
	- Tech: hardware, software
	- Morality: differentiating between good and evil / good and bad / right and wrong // 'moral' means good/right
	- Value judgement: judging usefulness/goodness/rightness // inextricably linked to morality

- Tech reflects the morals/worldview of its makers
	- This is not to say that the authors of that tech fully understand the implications of their value judgements!
	- Tech must be designed; we aren't interested in all possible programs, just a subset that have been chosen for reasons
	- What those reasons are must be reflected in the tech itself by definition of choice
	- These decisions are made based on human intuition - a people-oriented process - not computable and provable logic
	- Few, if any, tech pieces contain a complete morality

- Morals in tech are most visible in stuff designed to interact directly with people
	- infrastructure folks, you can rest assured that no matter how radical your politics, you won't overthrow the world
	- Examples:
		- IRC: people's interaction should ultimately be mostly controllable by those people themselves: ignore lists, client logging, client agnosticity, etc. Messages are ephemeral (server is tiny, etc) and realtime (or close to it)
		- Twitter: There is value in expressing thoughts in ~140 chars / thoughts that small are meaningful. 'Follow' implies context to some tweets (compare to, say, yik yak), but retweets give a ~log(# RTs) separation from context, and hashtags when used as intended strip all personal context entirely (but add content context). 
		- Whois says it's moral/ok for domain owners to pay for the freedom that having a domain entails with their privacy (or additional $$ for workarounds)
		- Wearables/Glass encode value in always being connected / massive data collection and storage, almost exclusively in a proprietary silo
	- Why is this? Probably because design decisions made 'close to' people (as opposed to having multiple layers stacked on top of it before an end user will touch it) involve understanding/imposing a view of how those people work/what they want/what is good for them and good overall
	- Something of a feedback loop: Tech design, user testing, feedback (user stories, use case diagrams, etc), updated tech

- Less firm statements / Vaguer ideas
	- A lot of tech encoding morality is very big (platforms, etc)
	- Is there some sort of tipping point wrt project size where including moral judgements becomes inevitable? Where is that tipping point?
	- AI /must/ include morals of its creators, whether explicitly encoded or implicitly designed - it is both big and very user-centric!

- TODO: Is 'how we use technology' materially different from that technology itself? (perhaps address at end)
	- Certain technologies preclude/heavily discourage non-standard uses (e.g. twitter and longform pieces, but see storify or whatev; also whois)
	- https://twitter.com/TinkerSec/status/614117147955077120
	- A lot of tech nowadays is SaaS-ish and therefore comes with the baggage of people/corporation, for better or worse (see both banning abusers and the whole real-name policy BS)
-->
</body>
</html>

