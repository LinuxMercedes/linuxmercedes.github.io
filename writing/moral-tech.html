<p>
It is common to see the opinion that "Technology is amoral" in technical discussions. 
At first glance, this argument seems sound: technology is basically just hammers made out of magnets, and hammers have both good and bad uses.
Therefore, technology must also not express a moral viewpoint; it is just a tool for its users to express their morals with. 
However, this argument goes wrong by greatly simplifying the situation. 
Technology is not necessarily amoral.
Many pieces of tech include at least part of the morals of their creators.
This will become more prevalent as we build more complex pieces of tech.
</p>

<p>
Technology is built to serve a purpose; we don't build things without some sort of reason. 
Therefore, the technology we have doesn't represent a random sampling from all possibly buildable things. 
This process of choosing what technology to build is how morality sneaks into our tech. 
Design decisions define the direction we take when developing technology; these decisions are ultimately based on our intuition, rather than purely technical merits. 
It's usually not obivous that these decisions are incorporating the authors' morals into their projects; as a result, few projects, if any, contain a complete or consistent morality. 
Anyone who has built complex systems know that the components interact in ways they would never have expected; this same complexity is responsible for self-contradictory moral judgements both in people and in their technology.
</p>

<p>
The morals embedded in technology are most visible in the tech that end users directly interact with.
Let's take a look at a few examples that will elucidate how these morals appear in technology.

IRC believes that peoples' interaction with it should be mostly controllable by those people themselves. 
As a result, users have their choice of client, with many client-side features including ignore lists and logging. 
Messages are (mostly) realtime and ephemeral-there is no replay, and therefore the server isn't required to do much more than forward messages to connected clients. <!-- How does this make the point I want to? -->

Twitter claims that there is value in expressing thoughts that are 140 or fewer characters long. 
The effect of this is that most tweets do not contain enough of an idea to be clearly taken out of context. 
Following people somewhat restores this context, as you have some sort of idea of what that person tweets about. 
However, retweets spread those tweets to people who don't follow the author, and hashtags remove tweets from their original context and place them into a new one (the feed for that hashtag). 

A third example: Whois believes that it is moral for domain owners to sacrifice their privacy for the freedom that having a domain brings (or, in certain cases, pay extra to maintain that privacy). 
You may argue that this is just a side effect of the age of when it was originally designed, but that is exactly the sort of subtle inclusion of morals we're looking at here. 
The designers worked under an assumption that their users would be primarily good or neutral. <!-- ...indicating something about their morality? -->

Finally, for a more hardware-centric example, consider wearable devices such as Google Glass, the FitBit, or [wearable cameras](https://www.fastcodesign.com/3047721/are-your-wearables-invading-someones-privacy).
All of these encode value in being constantly connected and monitored, usually with your data being stored in some proprietary data silo. 
In many of these cases, these devices signal not only this morality for the user, but also for people near the user who happen to be visible to the device. 

Why is it that it is particularly easy to point out morals encoded in user-facing technology?
I suspect it is due to the design decisions for those projects focusing primarily on people, instead of what the consuming layer of technology will want. 
These design decisions must involve choosing what is important to and good for people and good overall.

Something of a feedback loop is set up: developers make an initial design, perform user testing, gather feedback in the form of user stories, use case diagrams, and so forth, and go back to revise their design. <!-- What is the result of this? Patches to software morality? Introduced inconsistencies? -->
</p>


Thesis: Technology is not necessarily amoral. 

- Definitions
	- Tech: hardware, software
	- Morality: differentiating between good and evil / good and bad / right and wrong // 'moral' means good/right
	- Value judgement: judging usefulness/goodness/rightness // inextricably linked to morality

- Tech reflects the morals/worldview of its makers
	- This is not to say that the authors of that tech fully understand the implications of their value judgements!
	- Tech must be designed; we aren't interested in all possible programs, just a subset that have been chosen for reasons
	- What those reasons are must be reflected in the tech itself by definition of choice
	- These decisions are made based on human intuition - a people-oriented process - not computable and provable logic
	- Few, if any, tech pieces contain a complete morality

- Morals in tech are most visible in stuff designed to interact directly with people
	- infrastructure folks, you can rest assured that no matter how radical your politics, you won't overthrow the world
	- Examples:
		- IRC: people's interaction should ultimately be mostly controllable by those people themselves: ignore lists, client logging, client agnosticity, etc. Messages are ephemeral (server is tiny, etc) and realtime (or close to it)
		- Twitter: There is value in expressing thoughts in ~140 chars / thoughts that small are meaningful. 'Follow' implies context to some tweets (compare to, say, yik yak), but retweets give a ~log(# RTs) separation from context, and hashtags when used as intended strip all personal context entirely (but add content context). 
		- Whois says it's moral/ok for domain owners to pay for the freedom that having a domain entails with their privacy (or additional $$ for workarounds)
		- Wearables/Glass encode value in always being connected / massive data collection and storage, almost exclusively in a proprietary silo
	- Why is this? Probably because design decisions made 'close to' people (as opposed to having multiple layers stacked on top of it before an end user will touch it) involve understanding/imposing a view of how those people work/what they want/what is good for them and good overall
	- Something of a feedback loop: Tech design, user testing, feedback (user stories, use case diagrams, etc), updated tech

- Less firm statements / Vaguer ideas
	- A lot of tech encoding morality is very big (platforms, etc)
	- Is there some sort of tipping point wrt project size where including moral judgements becomes inevitable? Where is that tipping point?
	- AI /must/ include morals of its creators, whether explicitly encoded or implicitly designed - it is both big and very user-centric!

- TODO: Is 'how we use technology' materially different from that technology itself? (perhaps address at end)
	- Certain technologies preclude/heavily discourage non-standard uses (e.g. twitter and longform pieces, but see storify or whatev; also whois)
	- https://twitter.com/TinkerSec/status/614117147955077120
	- A lot of tech nowadays is SaaS-ish and therefore comes with the baggage of people/corporation, for better or worse (see both banning abusers and the whole real-name policy BS)
