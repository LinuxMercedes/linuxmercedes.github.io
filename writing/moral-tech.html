Thesis: Technology is not necessarily amoral. 

- Definitions
	- Tech: hardware, software
	- Morality: differentiating between good and evil / good and bad / right and wrong // 'moral' means good/right
	- Value judgement: judging usefulness/goodness/rightness // inextricably linked to morality

- Tech reflects the morals/worldview of its makers
	- This is not to say that the authors of that tech fully understand the implications of their value judgements!
	- Tech must be designed; we aren't interested in all possible programs, just a subset that have been chosen for reasons
	- What those reasons are must be reflected in the tech itself by definition of choice
	- These decisions are made based on human intuition - a people-oriented process - not computable and provable logic
	- Few, if any, tech pieces contain a complete morality

- Morals in tech are most visible in stuff designed to interact directly with people
	- infrastructure folks, you can rest assured that no matter how radical your politics, you won't overthrow the world
	- Examples:
		- IRC: people's interaction should ultimately be mostly controllable by those people themselves: ignore lists, client logging, client agnosticity, etc. Messages are ephemeral (server is tiny, etc) and realtime (or close to it)
		- Twitter: There is value in expressing thoughts in ~140 chars / thoughts that small are meaningful. 'Follow' implies context to some tweets (compare to, say, yik yak), but retweets give a ~log(# RTs) separation from context, and hashtags when used as intended strip all personal context entirely (but add content context). 
		- Whois says it's moral/ok for domain owners to pay for the freedom that having a domain entails with their privacy (or additional $$ for workarounds)
		- Wearables/Glass encode value in always being connected / massive data collection and storage, almost exclusively in a proprietary silo
	- Why is this? Probably because design decisions made 'close to' people (as opposed to having multiple layers stacked on top of it before an end user will touch it) involve understanding/imposing a view of how those people work/what they want/what is good for them and good overall
	- Something of a feedback loop: Tech design, user testing, feedback (user stories, use case diagrams, etc), updated tech

- Less firm statements / Vaguer ideas
	- A lot of tech encoding morality is very big (platforms, etc)
	- Is there some sort of tipping point wrt project size where including moral judgements becomes inevitable? Where is that tipping point?
	- AI /must/ include morals of its creators, whether explicitly encoded or implicitly designed - it is both big and very user-centric!

- TODO: Is 'how we use technology' materially different from that technology itself? (perhaps address at end)
	- Certain technologies preclude/heavily discourage non-standard uses (e.g. twitter and longform pieces, but see storify or whatev; also whois)
	- https://twitter.com/TinkerSec/status/614117147955077120
	- A lot of tech nowadays is SaaS-ish and therefore comes with the baggage of people/corporation, for better or worse (see both banning abusers and the whole real-name policy BS)
